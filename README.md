# transformer_from_scratch

This project is a minimal implementation of a Transformer Decoder-Only architecture (similar to GPT-style models), developed entirely from scratch using PyTorch and integrated with Lightning for training modularity. The model is designed to understand and predict sequences over a small custom vocabulary.

Training Details

Optimizer: torch.optim.Adam

Loss: Cross-entropy

Batch size: customizable via DataLoader

Framework: PyTorch Lightning
